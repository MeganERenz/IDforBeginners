{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "middle-coaching",
   "metadata": {},
   "source": [
    "# How do Neural Nets work?  \n",
    "\n",
    "Note:  I am assuming that students have Anaconda installed on their machine, along with the standard libraries that come along with it: numpy, matplotlib, and sklearn. Sklearn is not commonly used in real-world applications, but this was a design choice to allow people to try out and learn about our method without having to install more complicated software.  \n",
    "\n",
    "Neural networks are a very popular method in machine learning.  There are many different architectures, but to demonstrate how to prune them, we will start with one of the simplest, the single hidden layer network.  \n",
    "\n",
    "\n",
    "The central idea behind neural nets, especially historically, is that they are based on \"we think information is processed in the brain\" - hence the name.  Information is stored in the network in how we weight the connections between different neurons, and we train them to perform tasks by changing these weights. A single hidden layer neural network means that data passes in to a layer of neurons, and then the output of those neurons goes to a second (not hidden) layer for the output.  \n",
    "\n",
    "In our example here, we are going to train a network to identify the digits 0-9 from images of hand-written numbers.  \n",
    "\n",
    "We start by importing some simple data below.  Each piece of data is an 8x8 image of the hand written digits.  This is what we will call \"X\". Their labels we will call \"y\". Note that we split our data in to a \"train\" and a \"test\" set.  This lets us teach the network using some of our data, and then verify that it learned correctly based on data that it has never seen before. This is a way for us to know if the network learned how to recognize our digits, or if it simply memorized all the data it has already seen, but cannot perform well on new data.  The latter case is called over-fitting. \n",
    "\n",
    "\n",
    "(Select the cell below, and press \"shift\" and \"enter\" together to run the cell.)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "#load the dataset from the library.  \n",
    "digits = datasets.load_digits()\n",
    "\n",
    "#display some examples of the dataset.  \n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)\n",
    "\n",
    "n_samples = len(digits.images)\n",
    "    \n",
    "\n",
    "\n",
    "#flatten the images to a single column of 64.    These correspond to 64 features in each data point.   \n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-universe",
   "metadata": {},
   "source": [
    "Now we will define a neural network using sklearn, and train it to fit the data.  \n",
    "\n",
    "First, we need to quantify how close the network is to fitting the data.  We do this with a \"loss function\".  This can take several forms.  When we want the network to look at data and return a number (for example, looking at housing data and returning a predicted price), we will commonly use the Mean Squared Error (MSE).  This likely looks familiar if you have ever done curve fitting.  \n",
    "\n",
    "$MSE=\\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$\n",
    "\n",
    "where $y_i$ is the value predicted by the network, and $\\hat{y_i}$ is the true value.   We expect this to decrease as the network gets closer to predicting the correct value.  \n",
    "\n",
    "In this case, we want the network to sort the images into classes; for example, the first class would be all the written numbers the computer thinks are \"0\".  We encode the labels as probabilities; a label of \"0\" in our scheme would be encoded as [1,0,0,0,0,0,0,0,0], meaning that there is a probability of 1 that the hand-drawn digit is a zero, and a probability of zero that it is anything else (this is called \"one-hot encoding\").  We then train the network to predict these probabilities.  Our metric for how closely the probabilities match is called the log-loss:  \n",
    "\n",
    "$log$-$loss= \\sum_{i=1}^N \\sum_{j=1}^M p_j(x_i)log(q_j(x_i))$\n",
    "\n",
    "for each of $N$ images (also called data points; the $i^{th}$ image in the set is labelled as $x_i$), with $M$ possible classes, where $p_j(x_i)$ is the probability that the true label for data point $i$ is class $j$, and $q_j(x_i)$ is what the network predicts for the probability that $x_i$ is in class $j$. As an example of the probabilities, let's say that $x_{323}$ is a drawing of the number 7. $p_0(x_{323})$ should be zero; the one-hot encoding for \"0\" definitely does not describe the one-hot encoding for \"7\". $p_j(x_i)$ will always be either 0 or 1. $q_0(x_{323})$, on the other hand, is what the computer tells us that the probability of this hand-drawn 7 being classified as a 0 is. This could be anything between 0 and 1.\n",
    "\n",
    "The \"fit\" method uses something called gradient descent, which calculates the gradient of the loss function with respect to all of the parameters in the network, and then attempts to minimize the loss by changing the parameters to move \"downhill\" in the loss.  In addition to the loss, we can also look at the percentage of the data points which have their correct class predicted by the network.\n",
    "\n",
    "Run the cell below:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An MLP classifier is a one hidden layer neural network.  \n",
    "clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "print(\"Label for first data point:  {}\".format(y_train[0]))\n",
    "import numpy as np\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Class probabilities predicted by trained network:\\n {}'.format(clf.predict_proba(X_train[0].reshape(1, -1)).round(6)))\n",
    "\n",
    "\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test )))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-louis",
   "metadata": {},
   "source": [
    "We see that a relatively simple network with only two layers and 100 neurons in the hidden layer can perform well on this simple task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-edition",
   "metadata": {},
   "source": [
    "At their core, neural networks are mathematical models. We can show how to calcluate the output of the network as a series of matrix multiplications.  \n",
    "\n",
    "Each of the 100 neurons in the first layer has weights, which correspond to the \"features\" of the data (pixels, in this case).  Since the data points in this set are 8x8, there are 64 features and thus 64 weights.  We can find these weights from the model and write them as a matrix.\n",
    "\n",
    "The shape of the weight matrix is written as (weights per neuron, number of neurons).\n",
    "\n",
    "\n",
    "Run the code below.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of weight matrix\")\n",
    "print(clf.coefs_[0].shape)\n",
    "\n",
    "print(\"Weight matrix\")\n",
    "print(clf.coefs_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-ministry",
   "metadata": {},
   "source": [
    "The data can be written as a matrix, where each data point has 64 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of data matrix: \")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-place",
   "metadata": {},
   "source": [
    "The shape of the data matrix is written as (number of data points/images, number of features/pixels in each). Each feature will also have a value associated with it, to describe the color of the pixel. Since these images are greyscale, every feature will have a value from 0 (black) to 16 (white).\n",
    "\n",
    "Each of the neurons applies the weights to the data by multiplying the weight by the value of the feature in the data.  This looks like the matrix multiplication $z=xW$.\n",
    "\n",
    "Run the code below.  \n",
    "(using numpy, the @ symbol means matrix multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=clf.coefs_[0]\n",
    "z=X_train@W\n",
    "print(z)\n",
    "\n",
    "\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-member",
   "metadata": {},
   "source": [
    "Now we can see that each of the 100 neurons has an output for each of the 898 data points.  \n",
    "\n",
    "Each neuron also has a bias, which it adds to its output for each data point.    \n",
    "\n",
    "We can add this in numpy by broadcasting out the shape.  \n",
    "\n",
    "\n",
    "$Z=xW+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bias shape:\")\n",
    "print(clf.intercepts_[0].shape)\n",
    "print(\"bias:\")\n",
    "print(clf.intercepts_[0])\n",
    "b=clf.intercepts_[0]\n",
    "print(\"adding bias:\")\n",
    "Z=X_train@W+b\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-identity",
   "metadata": {},
   "source": [
    "So far, we have only done linear operations. However, our networks use a non-linear activation function that gets applied to the output which is called ReLU, or Rectification Linear Unit.  This is zero if the value of $Z$ is negative and leaves it alone if the value is positive. \n",
    "\n",
    "The value might be negative if the weights of a neuron and data point do not align well, i.e. their inner product is negative.  Intuitively, we do not want neurons that are very far away from \"matching\" the data point to be contributing to the classification.  The bias allows the network to decide how close the neuron and data point have to be to contribute.  \n",
    "\n",
    "We can implement this simply using the np.maximum function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=np.maximum(Z, 0)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-position",
   "metadata": {},
   "source": [
    "Note that all of the negative values have become zero. Applying the activation function can lead to a very different matrix than before the activation function, but since it is often non-linear, it may not commute with everything.  This will become important later.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-shelter",
   "metadata": {},
   "source": [
    "We can apply the same process to the second layer (the output layer). This layer has $M$ neurons since there are $M$ classes; the output need only tell us which image is in which class.  However, in this case we will use \"softmax\" $\\sigma$ as the activation function rather than ReLU.  \n",
    "\n",
    "$\\sigma (\\vec{x}) = \\frac{e^{x_i}}{\\sum_{j=1}^M e^{x_j}}$\n",
    "\n",
    "Note that this makes the predicted probabilities of all the classes sum to 1.  \n",
    "\n",
    "We will call the weights of the second layer $U$, and the biases of the second layer $\\beta$.  \n",
    "In matrix form:  \n",
    "\n",
    "$\\sigma ( ReLU( xW + b)U+ \\beta)$\n",
    "\n",
    "Putting this all together, \n",
    "(Run the code below) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "U=clf.coefs_[1]\n",
    "beta=clf.intercepts_[1]\n",
    "\n",
    "print(\"Shape of output of first layer\")\n",
    "print(Z.shape)\n",
    "print(\"Shape of second layer weights\")\n",
    "print(U.shape)\n",
    "\n",
    "secondLayer=Z@U+beta\n",
    "\n",
    "print(\"Output of second layer shape:  \")\n",
    "print(secondLayer.shape)\n",
    "def softmax(layer2):\n",
    "    e=np.e**layer2\n",
    "    normed=e/np.sum(e, axis=1)[:,None]\n",
    "    return normed\n",
    "output=softmax(secondLayer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-colony",
   "metadata": {},
   "source": [
    "To check that we have done this correctly, we will check the output on the first data point:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Network prediction\")\n",
    "print(clf.predict_proba(X_train[0].reshape(1, -1)).round(6))\n",
    "print(\"Network prediction from our matrix multiplications\")\n",
    "print(output[0].round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-luxembourg",
   "metadata": {},
   "source": [
    "Finally, we want to compare how well our network does on data it has seen (the training set) v.s. data it has not seen, the \"test set\".  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy: {}%\".format(round(clf.score(X_train, y_train)*100,2)))\n",
    "print(\"Test Accuracy: {}%\".format(round(clf.score(X_test, y_test)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-hampton",
   "metadata": {},
   "source": [
    "This is what we refer to as generalization, and will become important later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-crash",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
