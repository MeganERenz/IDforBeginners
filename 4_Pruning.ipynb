{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "potential-london",
   "metadata": {},
   "source": [
    "# How do we use interpolative decomposition and QR algorithms to prune neural nets in practice?  \n",
    "\n",
    "Now we can use our interpolative decomposition to prune our network.  First, we are going to train a network with 1000 hidden layer neurons on our digit classification task from the first tutorial.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets\n",
    "from copy import deepcopy\n",
    "import scipy.linalg\n",
    "#load the dataset from the library.  \n",
    "digits = datasets.load_digits()\n",
    "\n",
    "n_samples = len(digits.images)\n",
    "    \n",
    "#flatten the images to a single column of 64.     \n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n",
    "\n",
    "clf = MLPClassifier(1000,random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "print(\"Training Accuracy: {}%\".format(round(clf.score(X_train, y_train)*100,2)))\n",
    "print(\"Test Accuracy: {}%\".format(round(clf.score(X_test, y_test)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-union",
   "metadata": {},
   "source": [
    "Note that this performs better on the data than our original (smaller) network. In the very over-parameterized case, the network has enough parameters to completely memorize all of the data points in the training set.  In more complicated neural networks, especially in deep learning, the networks have more than enough parameters to memorize all of the training data.  However, they often do not over-fit to the extent that we might expect they would.  They can, in fact, start to \"generalize\" - meaning perform better than smaller nets on unseen data. \n",
    "\n",
    "This phenomenon is called double descent. Let's think about curve fitting: there is an old saying attributed to von Neumann, \"with four parameters I can fit an elephant, and with five I can make him wiggle his trunk\".  Fitting all of the data perfectly is called interpolation.  When we have the same number of parameters as we have data points, we can interpolate all of the points perfectly.  As anyone who has done curve fitting probably knows, this function is usually wild and does not fit the data well off of the data points.  However, as you keep adding more parameters, the fitting function has more different choices of parameter values that could fit the data.  When it has many choices of possible solutions, in some cases (particularly with under-determined least squares problems), we have reason to expect that it will pick the simplest choice, the minimal norm. \n",
    "\n",
    "\n",
    "There is also the idea of the \"Lottery Ticket Hypothesis\".  This is the idea that for some large enough network, there is a smaller subnet inside it that could train to perform very well on the data by itself.  However, it is empirically difficult to train small networks.  In general, the fewer parameters there are, the more the parameters have to move to perfectly classify all of the data.  Overall, in practice, networks are used that are vastly overparameterized.  Networks use many hidden layers, and can include millions of parameters.  \n",
    "\n",
    "Therefore, the networks that are made in practice often could be \"pruned\".  This means that we reduce the number of neurons in each layer.  For our example, we reduce the number of neurons in the one hidden layer.  We need to pick which neurons to eliminate, and compensate somehow.  Let's take a look at our matrices from tutorial 1.   \n",
    "\n",
    "We have several choices for pruning.  We could try to just prune the weight matrix $W$, preserving the largest neurons (or largest columns in the weight matrix).  This is called magnitude pruning.  This is one of the more commonly used methods.  However, this does not use any information that we have from our data.  We take a look at how this performs below, when pruning half of the neurons.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned=deepcopy(clf)  #want to keep original model.  \n",
    "W=pruned.coefs_[0]\n",
    "b=pruned.intercepts_[0]\n",
    "\n",
    "\n",
    "norms=np.sum(-W**2, axis=0)\n",
    "\n",
    "indx=np.argsort(norms)[0:500]\n",
    "\n",
    "W2=W[:,indx]\n",
    "\n",
    "b2=b[indx]\n",
    "pruned.coefs_[0]=W2\n",
    "pruned.intercepts_[0]=b2\n",
    "pruned.coefs_[1]=pruned.coefs_[1][indx,:]\n",
    "print(\"Pruned Training Accuracy: {}%\".format(round(pruned.score(X_train, y_train)*100,2)))\n",
    "print(\"Pruned Test Accuracy: {}%\".format(round(pruned.score(X_test, y_test)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-radiation",
   "metadata": {},
   "source": [
    "However, we want to include information from the data.  We can do that by choosing neurons so that we approximate the matrix $Z = ReLU(xW+b)$.  This also allows our pruning algorithm to have information about the activation function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned=deepcopy(clf)\n",
    "W=pruned.coefs_[0]\n",
    "print(W.shape)\n",
    "b=pruned.intercepts_[0]\n",
    "U=pruned.coefs_[1]\n",
    "k=500 #number of neurons kept\n",
    "\n",
    "Z=np.maximum(X_train@W+b, 0)\n",
    "print(\"shape of Z\")\n",
    "print(Z.shape)\n",
    "print(\"Z:\")\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-japan",
   "metadata": {},
   "source": [
    "The whole network can be described by the equation:  $\\sigma ( ZU + \\beta)$\n",
    "\n",
    "We are going to sub-select some columns of $Z$, and then update $U$ (the coefficients in the next layer) so that we can compensate for the columns that are missing.  By sub-selecting columns of $Z$, we are only using the contributions of certain neurons.  Therefore, we can prune away the rest.  This commutes with the activation function.  \n",
    "\n",
    "\n",
    "We take the interpolative decomposition,  $Z\\approx  Z_{I,:} T$.  $T$ is our interpolating matrix, and $Z_{I,:}$ is the sub-set of columns of $Z$.  We replace $Z$ in our equation:  \n",
    "\n",
    "$\\sigma ( Z_{I,:} T U+ \\beta)$\n",
    "\n",
    "Note that $T$ is just a matrix.  We can multiply on the right by $U$.  This is equivalent to updating the coefficients in the next layer to compensate for the missing neurons.  \n",
    "\n",
    "Now, we can use our pivoted QR decomposition from the third tutorial to calculate $T$.    \n",
    "\n",
    "$T=[I_k, R_{11}^{-1}R_{12}]\\Pi ^T$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "R, Pi = scipy.linalg.qr((Z), mode='r', pivoting=True)\n",
    "\n",
    "T = np.concatenate((\n",
    "        np.identity(k),\n",
    "        np.linalg.pinv(R[0:k, 0:k]) @ R[0:k, k:None]\n",
    "        ), axis=1)\n",
    "T = T[:, np.argsort(Pi)]\n",
    "\n",
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-radius",
   "metadata": {},
   "source": [
    "Then we simply multiply $U$ by $T$ to update the next layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(U.shape)\n",
    "nextLayer=T@U\n",
    "print(nextLayer.shape)\n",
    "pruned.coefs_[1]=nextLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-rental",
   "metadata": {},
   "source": [
    "Since we have sub-selected columns of $Z$, we can sub-select the neurons that made $Z$.  This corresponds to sub-selecting columns in $W$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.shape)\n",
    "Wk = W[:, Pi[0:k]]\n",
    "bk=b[Pi[0:k]]\n",
    "pruned.coefs_[0]=Wk\n",
    "pruned.intercepts_[0]=bk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-times",
   "metadata": {},
   "source": [
    "Let's see how it does!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pruned Training Accuracy: {}%\".format(round(pruned.score(X_train, y_train)*100,2)))\n",
    "print(\"Pruned Test Accuracy: {}%\".format(round(pruned.score(X_test, y_test)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-cotton",
   "metadata": {},
   "source": [
    "Now we have pruned half of the neurons, and kept most of the accuracy!  Let's compare this to a model of the same size run from scratch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfSmall = MLPClassifier(k,random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "print(\"Smaller Model Training Accuracy: {}%\".format(round(clfSmall.score(X_train, y_train)*100,2)))\n",
    "print(\"Smaller Model Test Accuracy: {}%\".format(round(clfSmall.score(X_test, y_test)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-beverage",
   "metadata": {},
   "source": [
    "While this is a single trial, and not statistically significant, we have shown that we can prune a larger network and keep most of the accuracy in a way that can be better than or competitive with a smaller model trained from scratch!\n",
    "\n",
    "Finally, let's look at the accuracy of the model.  We can actually make some guarantees about how much our pruning method will change what comes out.  \n",
    "\n",
    "Remember that the error on our interpolative decomposition as calculated by the column-pivoted QR was $||R_{22}||$. \n",
    "\n",
    "Let's check that below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scipy.linalg.norm(R[k:None, k:None], ord=2))\n",
    "\n",
    "Z=np.maximum(X_train@W+b, 0)\n",
    "Zprime=np.maximum(X_train@Wk+bk, 0)\n",
    "Zprime=(Zprime@T)\n",
    "\n",
    "print(scipy.linalg.norm(Z-Zprime, ord=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-wyoming",
   "metadata": {},
   "source": [
    "In the worst possible case, this error could be exactly aligned with $U$.  Therefore, we can use the Cauchy-Schwartz inequality:  $||R_{22}U||\\leq||U||\\ ||R_{22}||$\n",
    "\n",
    "Again, we can check this below.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uz=Z@U\n",
    "UZprime=Zprime@U\n",
    "print(\"norm of R22 U\")\n",
    "print(scipy.linalg.norm(Uz-UZprime, ord=2))\n",
    "print(\"norm of R22 times norm of U\")\n",
    "print(scipy.linalg.norm(U, ord=2)*scipy.linalg.norm(R[k:None, k:None], ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-tongue",
   "metadata": {},
   "source": [
    "Since the error $||R_{22}U||$ is much smaller than the maximum error, we have found that we do much better than the worst case!  \n",
    "\n",
    "Overall, we can easily extend this method to multi-hidden-layer networks by writing them as multiplications of matrices, and then applying interpolative decompositions.  Convolutional networks (which are more commonly used for images) can be more difficult, but are doable.  \n",
    "\n",
    "Finally, let's compare the compute times for each network:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "clf.score(X_train, y_train)\n",
    "print(\"Time for big network:  {}s\".format(time.time()-start))\n",
    "start=time.time()\n",
    "pruned.score(X_train, y_train)\n",
    "print(\"Time for pruned network:  {}s\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-miller",
   "metadata": {},
   "source": [
    "We have significantly reduced the time for our network to classify data points by pruning it and reducing the number of parameters!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-convenience",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
