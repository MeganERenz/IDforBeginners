{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "military-graph",
   "metadata": {},
   "source": [
    "# What is an interpolative decomposition?  \n",
    "Now that we have written our neural network as a string of matrix operations, we want to use this to prune our network.  For the next two tutorials, we are going to take a detour into some numerical linear algebra.  \n",
    "\n",
    "\n",
    "First, we are going to learn about matrix and vector norms, which gives us a metric to understand how \"big\" a matrix (and eventually the error we are adding to our networks) is.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-stylus",
   "metadata": {},
   "source": [
    "### What is a norm?  \n",
    "Note that I am choosing to work with only real-valued matrices and vectors, since those are the only kind we will see with these neural networks.  \n",
    "\n",
    "A vector norm is a function that assigns a real-valued length to a vector.  If you have taken physics classes, you are most likely familiar with at least one type of vector norm.  We call it the \"magnitude\".  For our purposes, the magnitude is called the 2-norm.  There are other types of norms, but the 2-norm is the most relevant here.  The 2-norm of a vector $x$ is denoted $||x|| $.  \n",
    "\n",
    "We will also be using norms with matrices.  Again, there are multiple types of norms that we could use.  We will be using the norm \"induced\" by the 2-vector-norm.  In simple terms, the 2-norm of a matrix is the most that a matrix could possibly \"stretch\" any possible vector that the matrix is applied to.  \n",
    "\n",
    "This can be written mathematically as:  \n",
    "$||A||=\\sup_{x\\in \\mathbb{R}^n} \\frac{||Ax||}{||x||}$  \n",
    "\n",
    "where $Ax$ is the vector we get when we multiply $A$ and $x$. This means that to find $||A||$, we have to use\n",
    "the vector $x$ which maximizes $\\frac{||Ax||}{||x||}$.\n",
    "\n",
    "Scipy has a simple method to compute the 2-norm of a matrix.  \n",
    "\n",
    "We see an example below.  Because the identity matrix cannot stretch or shrink any vector--every vector it is applied to is the same before and after application of the identity matrix--it has a 2-norm of 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "import numpy as np\n",
    "mat=np.eye(3)\n",
    "print(mat)\n",
    "scipy.linalg.norm(mat, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-sequence",
   "metadata": {},
   "source": [
    "However, if we change this matrix so that it is no longer the identity matrix, it will stretch a vector and the 2-norm will not be 1 anymore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[0,0]=3\n",
    "print(\"A\")\n",
    "print(mat)\n",
    "vec=np.array([1,1,1])\n",
    "print(\"||Ax||/||x||:\")\n",
    "print(scipy.linalg.norm(mat@vec, ord=2)/scipy.linalg.norm(vec, ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-attendance",
   "metadata": {},
   "source": [
    "In the example above, we did not choose the vector $x$ which would be stretched the most by the matrix $A$ (which is the vector that maximizes $\\frac{||Ax||}{||x||}$), and so the number on the last output line is not the norm.  \n",
    "\n",
    "The vector that would be stretched the most (while having its own vector norm as 1) is called a right singular vector (particularly, the singular vector which corresponds to the largest singular value). This is the vector we would have to find to calculate $||A||$. The amount that it is stretched by is called the largest singular value. This is the basis of the singular value decomposition.   \n",
    "\n",
    "For our purposes though it is sufficient to understand how the norm works and to know that the most that a matrix could stretch any vector is by the largest singular value.  For our example matrix $A$, the right singular vector with the largest singular value is (1,0,0).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-family",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vec=np.array([1,0,0])\n",
    "print(\"||Ax||/||x||:\")\n",
    "print(scipy.linalg.norm(mat@vec, ord=2)/scipy.linalg.norm(vec, ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-profile",
   "metadata": {},
   "source": [
    "So 3.0 is the 2-norm of our matrix $A$.  \n",
    "\n",
    "There are a few other things which are important to know about norms, primarily that they follow the Cauchy-Schwartz inequality.  This means that when multiplying matrices, \n",
    "\n",
    "$||AB|| \\leq ||A||\\ ||B|| $\n",
    "\n",
    "and  \n",
    "\n",
    "$||A+B|| \\leq ||A||+||B|| $\n",
    "\n",
    "the latter of which is called the triangle inequality.\n",
    "\n",
    "\n",
    "This is shown in many numerical linear algebra textbooks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-blade",
   "metadata": {},
   "source": [
    "### The interpolative decomposition \n",
    "\n",
    "Oftentimes matrices need to be made smaller. If we think of a matrix as a collection of columns, this means selecting only a few columns to explicitly keep and setting up a way to figure out (or approximate) the unselected columns of the matrix later.\n",
    "\n",
    "Interpolative decomposition is the way to do this. We select a set of columns and then create a linear combination of the selected columns to represent the columns that we left out. That way we only have a few of the matrix's columns 'stored', but can still access all of them (approximately). \n",
    "\n",
    "\n",
    "Our goal is to approximate $A$ as $A_{:,I} T$, where $A_{:,I}$ is a matrix made up of a subset of columns of $A$ with $I$ as the column index. $T$ is the interpolation matrix, which does not change the columns of $A$ that were kept (except for re-arranging them into the right order) and uses linear combinations of those columns to approximate the columns that were left out.  \n",
    "\n",
    "If our interpolative decomposition is successful, we would like the values in $T$ to be reasonably small (less than 2), and for the difference between our interpolated matrix and $A$ to be small as well (relative to the size of $A$):   \n",
    "\n",
    "$||A-A_{:,I} T|| \\leq \\epsilon ||A||$\n",
    "\n",
    "This is called an $\\epsilon$-accurate interpolative decomposition.  Obviously, we would like $\\epsilon$ to be small, since that means our approximation is relatively close to the matrix we are approximating.  We would also like to use the smallest number of columns possible to achieve this accuracy--that is, make $A_{:,I}$ as small as we can.  \n",
    "\n",
    "This is not an easy wishlist to fulfill.  Finding the optimal ones in practice is hard.  \n",
    "\n",
    "In the next tutorial, we will show one method which gives us a good interpolative decomposition in practice most of the time.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
